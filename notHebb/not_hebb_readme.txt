
    1. learning_rate (Коэффициент обучения):
    Коэффициент обучения регулирует, насколько сильно веса и порог изменяются при каждом обновлении.
    В контексте алгоритма Хебба это значение контролирует "скорость" обучения.
    Больший коэффициент обучения:

    Если вы увеличите learning_rate, веса будут обновляться более резко. Это может ускорить процесс обучения, но есть риск,
    что модель будет перешагивать через минимумы ошибок, не сходясь к оптимальному решению.
    Также возможна нестабильность, если ошибка слишком большая.
    Например, если вы увеличите learning_rate до 0.1, модель будет быстро корректировать веса, но может "перепрыгнуть"
    через оптимальные значения.

    Меньший коэффициент обучения:
    Если вы уменьшите learning_rate, обновления весов будут происходить медленно. Это может замедлить процесс обучения,
    но поможет избежать чрезмерных скачков и улучшить стабильность обучения.
    Например, если вы установите learning_rate на 0.001, модель будет обновлять веса медленно, но будет более устойчиво
    сходиться к решению.

    Примеры эффектов:
    learning_rate = 0.1: Модель будет учиться быстрее, но рискует не стабилизироваться.
    learning_rate = 0.001: Модель будет учиться медленно, но будет более стабильной.

    2. initial_bias (Начальный порог):
    Порог (bias) влияет на то, как сильно каждый входной сигнал будет изменять предсказание. Начальный порог также влияет на
    скорость обучения, поскольку это первое значение, с которым модель начинает работу. Его значение может существенно повлиять на
    начальную фазу обучения.
    Больший начальный порог:
    Если начальный порог слишком велик, это может привести к сильному смещению в предсказаниях модели с самого начала,
    что усложнит корректировку весов в дальнейшем. Однако это также может помочь в том, чтобы модель быстрее начала учиться,
    если начальное смещение было близким к правильному значению.
    Например, если initial_bias = 10, это может создать значительное начальное смещение, которое повлияет на все вычисления.
    Меньший начальный порог:

    Если начальный порог слишком мал (например, близкий к 0), это может привести к медленному началу обучения, особенно
    если входные данные плохо откалиброваны.
    Однако, если начальный порог близок к оптимальному, это может помочь быстрее найти правильное решение.
    Примеры эффектов:

    initial_bias = 10: Порог будет сильно влиять на первые предсказания, возможно, корректируя вес с первых шагов.
    initial_bias = 0.05: Модель начнёт с более нейтральным значением порога, что может замедлить начало, но постепенно
    сойдется к лучшему решению.
